# Udacity-Machine-Learning-Engineer-Nanodegree

The purpose of this repository is to serve as a portfolio of my work associated with the Udacity Machine Learning Engineer Nanodegree.  A brief overview of the goal of each project and the primary tools used.  The individual notebook or markdown files each contain a project overview or introduction providing additonal project context.

## Project: Predicting Boston Housing Prices (Model Evaluation & Validataion)
**Goal:** Build an optimal model based on a statistical analysis with the tools available. This model will then be used to estimate the best selling price for your clients' homes. **Tools Used:** scikit-learn, Python, MatplotLib, Pandas, Numpy

## Project: Finding Donors for CharityML (Supervised Learning) 
**Goal:** Evaluate and optimize several different supervised learners to determine which algorithm will provide the highest donation yield while also reducing the total number of letters being sent. **Tools Used:** scikit-learn, Python, MatplotLib, Seaborn, Pandas, Numpy

## Project: Dog Breed Claasifier (Deep Learning - Convolutional Neural Networks)
**Goal:** Use a Convolutional Neural Networks (CNN) and build a pipeline to process real-world, user-supplied images. Given an image of a dog, create an algorithm to identify an estimate of the canine’s breed. If supplied an image of a human, the code will identify the resembling dog breed.**Tools Used:** Amazon Web Services, scikit-learn, Keras, MatplotLib, Seaborn, Pandas, Numpy. _**Note: This is a very large and performance intensive project which required the use of a AWS GPU-enabled server, and extremely large set of image data files.  The notebook file and code are viewable but the code is not executable without the additional data files.**_

## Project: Creating Customer Segments (Unsupervised Learning)
**Goal:** use unsupervised learning techniques to see if any similarities exist between customers, and how to best segment customers into distinct categories. **Tools Used:** scikit-learn, MatplotLib, Seaborn, Pandas, Numpy, Regression

## Project: Teach a Quadcopter to Fly (Reinforcement Learning)
**Goal:**  Design an agent that can fly a quadcopter, and then train it using a reinforcement learning algorithm.  The selected algorithm is Deep Deterministic Policy Gradients or DDPG and an actor-critic method. **Tools Used:** Amazon Web Services, scikit-learn, Keras, MatplotLib, Seaborn, Pandas, Numpy  

## Project: Capstone Project Proposal - Loan Default Prediction (Supervised Unbalanced Binary Classification)
**Goal:** A project proposal encompasses seven key points of the final Machine Learning Capstone prior to implementation: The project's domain background — the field of research where the project is derived.  A problem statement — a problem being investigated for which a solution will be defined.  The datasets and inputs — data or inputs being used for the problem.  A solution statement — a the solution proposed for the problem given.  A benchmark model — some simple or historical model or result to compare the defined solution to.  A set of evaluation metrics — functional representations for how the solution can be measured.  An outline of the project design — how the solution will be developed and results obtained.

## Project: Capstone Project - Loan Default Prediction (Unbalanced Classification)
**Overview:** In Phase 1 of the project, the data was obtained and then analyzed and inspected for anomalies. A number of analogies such as skewed data were transformed or corrected. A handful of new features were extracted, label encoding was used on small categories and onehot encoding on the remaining categorical features. Numerical features were scaled. The imbalanced dataset was addressed using class weights and with an attempt to use SMOTE. Several classification algorithms were selected, implemented, and optimized using grid/random search. The selected model was then subjected to feature selection and re-executed in search of an improvement. Finally, in Phase 2 the additional datasets were aggregated and merged and a subset of the steps were performed again on the new consolidated dataset and then the selected models were re-applied using feature importance/selection. The selected classifiers with optimal parameters were fit to the processed data and corresponding labels. Predictions were made using the unlabeled test data, and probability of class membership was generated as a file output and submitted to Kaggle. The Kaggle calculated AUC was returned for the file submitted. This process was repeated several times in search of the best possible score. **Models and Tools Used:**  Amazon Web Services, scikit-learn, Keras, Logistic Regression, Decision trees, Random Forests, Artificial Neural Networks, XGBoost. Python, Pandas, Numpy, Seaborn & MatPlotLibe used for initial Data Wrangling and plotting.  _**Note: This is a very large and performance intensive project which required the use of a AWS GPU-enabled server, and extremely data files.  The notebook file and code are viewable but the code is not executable without the additional data files.  Please refer to the Capstone report .PDF for key code snippets and explantions including a report writeup of the overall project.**_
 
